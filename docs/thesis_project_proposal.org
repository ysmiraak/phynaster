#+TITLE: The road first taken: Optimizing choices in logical inference with neural network
#+DATE: May 30, 2020
#+AUTHOR: Kuan Yu

This is a project proposal for my master's thesis in the [[http://www.ling.uni-potsdam.de/cogsys/][Cognitive Systems]] program at the University of Potsdam.
First draft on May 30, 2020.

* Introduction

symbolic intelligence with connectionist brain

* Background

** Neural guided logical inference

** The miniKanren language

*** Basic examples

#+BEGIN_SRC scheme
(run* [q] (== 1 q))
;; (1)
#+END_SRC

#+BEGIN_SRC scheme
(run* [q]
  (conde
   [(== 0 q)]
   [(== 1 q)]))
;; (0 1)
#+END_SRC

#+BEGIN_SRC scheme
(define (appendo l s out)
  (conde
   [(== '() l) (== s out)]
   [(fresh [a d res]
      (== (cons a d) l)
      (== (cons a res) out)
      (appendo d s res))]))

(run* [p q] (appendo p q '(1 2 3 4)))
;; ((() (1 2 3 4))
;;  ((1) (2 3 4))
;;  ((1 2) (3 4))
;;  ((1 2 3) (4))
;;  ((1 2 3 4) ()))
#+END_SRC

Here is a faithful albeit non-idiomatic translation in Prolog.

#+BEGIN_SRC prolog
append(L,S,Out) :- L = [], S = Out.
append(L,S,Out) :- L = [A|D], Out = [A|Res], append(D,S,Res).

?- append(P,Q,[1,2,3,4]).
%@ P = [],
%@ Q = [1, 2, 3, 4] ;
%@ P = [1],
%@ Q = [2, 3, 4] ;
%@ P = [1, 2],
%@ Q = [3, 4] ;
%@ P = [1, 2, 3],
%@ Q = [4] ;
%@ P = [1, 2, 3, 4],
%@ Q = [] ;
%@ false.
#+END_SRC

#+BEGIN_SRC scheme
(run* [r] (appendo '(1 2) '(3 4) r))
;; ((1 2 3 4))
#+END_SRC

the importance of recursion

#+BEGIN_SRC scheme
(define (nato n)
  (conde
   [(== 'z n)]
   [(fresh [m]
      (== (cons 's m) n)
      (nato m))]))

(run 4 [q] (nato q))
;; (z (s . z) (s s . z) (s s s . z))
#+END_SRC

#+BEGIN_SRC scheme
(define (addo m n o)
  (conde
   [(== 'z m) (== n o)]
   [(fresh [l p]
      (== (cons 's l) m)
      (== (cons 's p) o)
      (addo l n p))]))

(run* [q] (addo '(s . z) '(s . z) q))
;; ((s s . z))
#+END_SRC

*** Why miniKanren?

- general recursion \`a la Prolog
- it's an extensible embedded language, with numerous extensions
- minimal core (microKanren), easy to implement and extend
- breath first search instead of depth first search, with one line of change

#+BEGIN_SRC scheme
(define (bind $ g)
  (cond
    ((null? $) mzero)
    ((procedure? $) (lambda () (bind ($) g)))
    (else (mplus (g (car $)) (bind (cdr $) g)))))
#+END_SRC

** Attention network

* Project proposal

** Where magic happens

We start with an [[https://github.com/ysmiraak/phynaster/blob/9c0e813833ed6bb1c78f89e46e249a4d6ccc9017/src/phynaster/logic2.clj#L85-L92][modified implementation]] of miniKanren.
Of crucial interests are the following two aspects of modifications.

*** Flattened disjunctives

#+BEGIN_SRC clojure
(| g1
   (| (| g4
         g5)
      g3)
   g2)
#+END_SRC

*** Descriptive goals

We added descriptions to the goals in the form of symbolic expressions,
which are used by the neural network model for predicting the probability of success.

The symbolic descriptions are constructed inductively as follows.

#+BEGIN_EXAMPLE
Desc =
| (ConstraintName Data ...)
| (& Desc ...)
| (| Desc ...)

ConstraintName =
| ==
| !=
| domain
| ...

Data =
| Constant
| Variable
| List

List = (list* Data ... Tail)
Tail = nil | Variable
#+END_EXAMPLE

The symbols in the language are from either
a finite pool of constants,
a finite pool of variables,
or a fixed pool of special symbols including constraint names, =&=, =|=, =list*=, and =nil=.

The constants and variables are atomic data.
Compound data are represented as lists.
Here we use a special list format =list*=,
which is a hybrid between =cons= and =list=.
On one hand,
the =cons= cell representation (e.g. =(cons a (cons b (cons c nil)))=) results in deeply nested expressions,
making it slower for the neural network to process,
and the gradient flow during backpropogation difficult.
On the other hand,
the flatten =list= representation (e.g. =(list a b c)=) lacks the ability to have a logical variable sitting as the tail,
which removes the usefulness of lists for representing partially generated expressions during logical inference.
Therefore, we adopt a hybrid representation =list*=,
translating =(cons a (cons b (cons c tail)))= as =(list* a b c tail)=
where =tail= is either =nil= or a logical variable.
This method gives us an efficient list representation retaining its inductive structure
which makes it expressive enough for representating arbitrary algebraic expressions.

A goal description is a symbolic expression whose head is a special symbol representing the goal constructor.
Goal constructors include constraints (equality ~==~, inequality ~!=~, finite domain =domain=, etc.)
and connectives (=&= and =|=).


zzzz TODO change the example, avoid recursion problem, make one branch fail

As an example, consider the first two leaf nodes in infering =:q= as a natural number,
namely that =:q= unifies with either ='z= or ='(s z)=.
Here we use keywords =:p= and =:q= as logical variables,
and quoted symbols ='s= and ='z= as constants.

#+BEGIN_SRC clojure
(| (== 'z :q)
   (& (== (list* 's :p) :q)
      (== (list* 'z ()) :p)))
#+END_SRC

** Neural network

Data = Const | LVar | List Data

*** Embedding: Data types

**** Constants

**** Varaibles

**** Lists

the inductive structure of lists make them expressive enough for represent any algebraic expressions

*** Embedding: Goals

- equality   : [Data] -> Goal
- inequality : [Data] -> Goal
- domain     : [Data] -> [LVar] -> Goal
- conjunction : [Goal] -> Goal
- disjunction : [Goal] -> Goal

(=== a b c)
(=!= a b c)
(domain a b c)
((domain a b c) p q r)
(| p q r)
(& p q r)
(list* a b c tail)

*** Binary prediction

** Evaluation
