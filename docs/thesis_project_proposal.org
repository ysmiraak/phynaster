#+TITLE: The road first taken: Optimizing choices in logical inference with neural network
#+DATE: May 30, 2020
#+AUTHOR: Kuan Yu

This is a project proposal for my master's thesis in the [[http://www.ling.uni-potsdam.de/cogsys/][Cognitive Systems]] program at the University of Potsdam.
First draft on May 30, 2020.

* Introduction

symbolic intelligence with connectionist brain

* Background

** Neural guided logical inference

** The miniKanren language

*** Basic examples

#+BEGIN_SRC scheme
(run* [q] (== 1 q))
;; (1)
#+END_SRC

#+BEGIN_SRC scheme
(run* [q]
  (conde
   [(== 0 q)]
   [(== 1 q)]))
;; (0 1)
#+END_SRC

#+BEGIN_SRC scheme
(define (appendo l s out)
  (conde
   [(== '() l) (== s out)]
   [(fresh [a d res]
      (== (cons a d) l)
      (== (cons a res) out)
      (appendo d s res))]))

(run* [p q] (appendo p q '(1 2 3 4)))
;; ((() (1 2 3 4))
;;  ((1) (2 3 4))
;;  ((1 2) (3 4))
;;  ((1 2 3) (4))
;;  ((1 2 3 4) ()))
#+END_SRC

Here is a faithful albeit non-idiomatic and clumsy translation in Prolog.

#+BEGIN_SRC prolog
append(L,S,Out) :- L = [], S = Out.
append(L,S,Out) :- L = [A|D], Out = [A|Res], append(D,S,Res).

?- append(P,Q,[1,2,3,4]).
%@ P = [],
%@ Q = [1, 2, 3, 4] ;
%@ P = [1],
%@ Q = [2, 3, 4] ;
%@ P = [1, 2],
%@ Q = [3, 4] ;
%@ P = [1, 2, 3],
%@ Q = [4] ;
%@ P = [1, 2, 3, 4],
%@ Q = [] ;
%@ false.
#+END_SRC

#+BEGIN_SRC scheme
(run* [r] (appendo '(1 2) '(3 4) r))
;; ((1 2 3 4))
#+END_SRC

the importance of recursion

#+BEGIN_SRC scheme
(define (nato n)
  (conde
   [(== 'z n)]
   [(fresh [m]
      (== (cons 's m) n)
      (nato m))]))

(run 4 [q] (nato q))
;; (z (s . z) (s s . z) (s s s . z))
#+END_SRC

#+BEGIN_SRC scheme
(define (addo m n o)
  (conde
   [(== 'z m) (== n o)]
   [(fresh [l p]
      (== (cons 's l) m)
      (== (cons 's p) o)
      (addo l n p))]))

(run* [q] (addo '(s . z) '(s . z) q))
;; ((s s . z))
#+END_SRC

*** Why miniKanren?

- general recursion Ã  la Prolog
- it's an extensible embedded language, with numerous extensions
- minimal core (microKanren), easy to implement and extend
- breath first search instead of depth first search, with one line of change

#+BEGIN_SRC scheme
(define (bind $ g)
  (cond
    ((null? $) mzero)
    ((procedure? $) (lambda () (bind ($) g)))
    (else (mplus (g (car $)) (bind (cdr $) g)))))
#+END_SRC

** Attention network

* Project proposal

** Where magic happens

We start with an [[https://github.com/ysmiraak/phynaster/blob/9c0e813833ed6bb1c78f89e46e249a4d6ccc9017/src/phynaster/logic2.clj#L85-L92][modified implementation]] of miniKanren.
Of crucial interests are the following two aspects of modifications.

*** Flattened disjunctives

#+BEGIN_SRC clojure
(| g1
   (| (| g4
         g5)
      g3)
   g2)
#+END_SRC

*** Descriptive goals

We added descriptions to the goals in the form of symbolic expressions,
which are used by the neural network model for predicting the probability of success.

The symbolic descriptions are constructed inductively as follows.

#+BEGIN_EXAMPLE
Description =
| (ConstraintName Data ...)
| (& Description ...)
| (| Description ...)

ConstraintName =
| ==
| !=
| domain
| ...

Data =
| Constant
| Variable
| List

List = (list* Data ... Tail)
Tail = nil | Variable
#+END_EXAMPLE

The symbols in the language are from either
a finite pool of constants,
a finite pool of variables,
or a fixed pool of special symbols including constraint names, =&=, =|=, =list*=, and =nil=.

The constants and variables are atomic data.
Compound data are represented as lists.
Here we use a special list format =list*=,
which is a hybrid between =cons= and =list=.
On one hand,
the =cons= cell representation (e.g. =(cons a (cons b (cons c nil)))=) results in deeply nested expressions,
making it slower for the neural network to process,
and the gradient flow during backpropogation difficult.
On the other hand,
the flatten =list= representation (e.g. =(list a b c)=) lacks the ability to have a logical variable sitting as the tail,
which removes the usefulness of lists for representing partially generated expressions during logical inference.
Therefore, we adopt a hybrid representation =list*=,
translating =(cons a (cons b (cons c tail)))= as =(list* a b c tail)=
where =tail= is either =nil= or a logical variable.
This method gives us an efficient list representation retaining its inductive structure
which makes it expressive enough for representating arbitrary algebraic expressions.

A goal description is a symbolic expression whose head is a special symbol representing the goal constructor.
Goal constructors include constraints (equality ~==~, inequality ~!=~, finite domain =domain=, etc.)
and connectives (=&= and =|=).

*** Example

Here is a sample goal description.
For readability, we use keywords (=:p=, =:q=, and =:r=) to represent logical variables.

#+BEGIN_SRC clojure
(& (== (list* 1 :p :r) :q)
   (| (& (== :r nil)
         (== :p 2))
      (== :p 4))
   (!= (list* 1 2 nil) :q))
#+END_SRC

This global goal contains one disjunctive with two branches.
After initialization, the inference board contains two thunks,
each containing its current local goal with the respective description as follows.

#+BEGIN_SRC clojure
;; thunk 1
(& (== :r nil)
   (== :p 2))
;; thunk 2
(== :p 4)
#+END_SRC

We will run a neural network on the global goal description,
as well as the local goal descriptions,
to produce embedded representations in a shared vector space.
For each thunk,
the global embedding and the local embedding are then combined in the final output layer to predict its success rate.

A useful neural network should rank thunk 2 higher than thunk 1,
since thunk 1 will fails due to the inequality constraint,
but thunk 2 will produce one successful result.

#+BEGIN_SRC clojure
{:p 4, :q (1 4 . ?0), :r ?0}
#+END_SRC

Namely,
=:p= unifies with the constant =4=,
=:q= unifies with the list =(list* 1 4 :r)=,
and =:r= remains unbound.

*** Problem: recursive goals

Consider =nato=, a recursively defined goal.

#+BEGIN_SRC scheme
(define (nato n)
  (| (== 'z n)
     (fresh [m]
       (== (list* 's m) n)
       (nato m))))
#+END_SRC

While this goal can be constructed and executed without termination problems,
due to its recursive part being contained in a thunk,
the description of this goal, however, is an infinite expression.

#+BEGIN_SRC clojure
(| (== 'z :n)
   (& (== (list* 's :n') :n)
      (| (== 'z :n')
         (& (== (list* 's :n'') :n')
            (| (== 'z :n'')
               (& (== (list* 's :n''') :n'')
                  ...))))))
#+END_SRC

We propose an ad hoc treatment to this problem:
We replace the recusive part of the description with a special expression =(rec Variable ...)=
where =rec= is a new special symbol.

This way, the goal =(nato :n)= has the following global description.

#+BEGIN_SRC clojure
(| (== 'z :n)
   (& (== (list* 's :n') :n)
      (rec :n')))
#+END_SRC

And the subgoal =(rec :n')= when dethunked will have the following description.

#+BEGIN_SRC clojure
(| (== 'z :n')
   (& (== (list* 's :n'') :n')
      (rec :n'')))
#+END_SRC

** Neural network

Data = Const | LVar | List Data

*** Embedding: Data types

**** Constants

**** Varaibles

**** Lists

the inductive structure of lists make them expressive enough for represent any algebraic expressions

*** Embedding: Goals

- equality   : [Data] -> Goal
- inequality : [Data] -> Goal
- domain     : [Data] -> [LVar] -> Goal
- conjunction : [Goal] -> Goal
- disjunction : [Goal] -> Goal

(=== a b c)
(=!= a b c)
(domain a b c)
((domain a b c) p q r)
(| p q r)
(& p q r)
(list* a b c tail)

*** Binary prediction

** Evaluation
