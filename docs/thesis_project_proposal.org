#+TITLE: The road first taken: Optimizing choices in logical inference with neural network
#+DATE: June 08, 2020
#+AUTHOR: Kuan Yu

This is a project proposal for my master's thesis in the [[http://www.ling.uni-potsdam.de/cogsys/][Cognitive Systems]] program at the University of Potsdam.
First draft on May 30, 2020.

* Introduction

Logical inference commonly involves a backward search process,
where we check each rule from which the goal can be derived.
The plurality of rules creates branches in the search tree,
and the complexities of nested branches multiply.
When we come to a branching node, we face choices.
Some choices eventually lead us to successful results,
and others to dead ends,
or worse, to infinite loops.

If we wish to retrieve all possible results,
an exhaustive search is unavoidable,
in which case the choices are irrelevant,
since all branches must be traversed, preferably in parallel.
However, in practice, we often need just one successful result which satisfies our goal.
Moreover, in reality, making a wrong choice has far more severe consequences than the computation cost.
For the time being, we guide machines with well-defined goals whose results can be simulated,
but eventually they will have to learn by interacting with the real world.

We propose to augment a general logical inference engine with a neural network responsible for making choices:
a symbolic intelligence with a connectionist brain.
This project will be a rudimentary exploration into this idea.
We will train a neural network in a supervised manner to predict the success rate of choices for prioritization
in order to reduce the breadth and the depth of the subtree traversed before the first successful result is found.

This project aims at laying a solid and extensible groundwork for the following future research directions.
- Replacing supervised training with reinforcement learning, so that the program learns to make better choices through interactions.
- Replacing success rate with more complicated objectives for which the choices are to be optimized.

* Background

** Neural guided logical inference

[[https://arxiv.org/abs/1701.06972][Loos et al. (2017)]] demonstrated the usefulness of deep neural networks for guiding first-order logic proof search,
where the networks were used for ranking clause selections in the [[https://wwwlehre.dhbw-stuttgart.de/~sschulz/E/E.html][E theorem prover]],
which significantly reduced the number of proof search steps.
They experimented with convolutional networks and tree LSTM networks.
[[https://arxiv.org/abs/1805.07563][Kaliszyk et al. (2018)]] successfully applied reinforcement learning for first-order theorem proving,
using tableau calculus for finding refutational proofs.
[[https://arxiv.org/abs/1811.00796][Kusumoto et al. (2018)]] trained graph networks with reinforcement learning
for guiding the construction of proof trees in intuitionistic propositional logic using sequent calculus,
which outperformed Coq's /tauto/ tactic.
These works have shown the power of neural guidance for logical inference in limited contexts.

[[https://arxiv.org/abs/1805.10872][Manhaeve et al. (2018)]] introduced /DeepProbLog/
which integrated neural predicates, e.g. images, into probabilistic logic programming.
[[https://arxiv.org/abs/1901.04195][Marra et al. (2019)]] used graph networks in probabilistic graphical models
with MAP inference for modeling logical reasoning.
These works explored the integration of probabilistic modeling and logical reasoning.
The general direction of their approach, however, diverges from ours.
Our goal is to harness the power of statistical methods
while retaining the definitive nature of symbolic reasoning.

Our approach is heavily inspired by [[https://arxiv.org/abs/1809.02840][Zhang et al. (2018)]]
who showed the benefits of neural guidance for program synthesis.
They experimented with recurrent LSTM networks and gated graph networks
for synthesizing /Scheme/ programs using the /miniKanren/ logic programming system.
The neural networks were trained with supervised curriculum and scheduled sampling
to predict the ground-truth optimal candidates at each step for expanding the partially synthesized program.
We discuss the following details in their work from which [[sec-proposal][our proposal]] differs.

- They used neural networks for embedding constraints which depended on the target synthesis language.
  The synthesis engine consisted of a few relational programs (e.g. =evalo=, =lookupo=)
  each containing several candidate branches of constraints.
  The weights of the networks were specialized for each relational program,
  and the model predicted a distribution over the candidates in that program.
- Their recurrent model was more efficient and performant, but it discarded variable identities.
- During test time, they used a greedy policy.  Only the best predicted candidate was chosen.

** The /miniKanren/ language

We would like to use [[http://minikanren.org/][/miniKanren/]] as the basis of the logic programming system,
for the following reasons.

- It is an embedded domain specific language with a [[http://webyrd.net/scheme-2013/papers/HemannMuKanren2013.pdf][minimal core]] which is easy to implement.
  It has been implemented in over 40 host languages,
  covering almost every common platforms.
- It is a general logic programming system Ã  la /Prolog/,
  based on unification resolution,
  supporting recursion or whatever the host language has to offer.
- It is an extensible system, with numerous extensions
  such as [[http://www.schemeworkshop.org/2011/papers/Alvis2011.pdf][constraint programming]], [[http://webyrd.net/alphamk/alphamk_workshop.pdf][nominal logic]], and [[https://github.com/webyrd/probKanren][probabilistic logic programming]].
- It has well-understood [[http://okmij.org/ftp/papers/LogicT.pdf][computational effects]].
  By default it performs an interleaving breadth-first search,
  but this behavior can be easily changed to depth-first search or [[http://webyrd.net/scheme-2013/papers/Swords2013.pdf][guided search]].

Here we introduce the [[https://github.com/webyrd/miniKanren-with-symbolic-constraints][canonical /Scheme/ version]] of the language with some basic examples followed by explanations.

#+BEGIN_SRC scheme
(run* [q] (== 1 q))
;; (1)
#+END_SRC

- The macro =run*= takes a list of query variables and one or more goal expressions,
  and returns the list of all results satisfying the goals.
- The goal constructor ~==~ is used for unification.

#+BEGIN_SRC scheme
(run* [p q]
  (conde
   [(== 0 p) (== 0 q)]
   [(== 1 q) (== 2 q)]
   [(== 3 p) (== 3 q)]))
;; ((0 0) (3 3))
#+END_SRC

- The macro =conde= takes a number of goal lists,
  creates the conjunctions of goals within each list,
  and returns the goal of their disjunction.

#+BEGIN_SRC scheme
(run* [q]
  (fresh [q]
    (== 0 q))
  (== 1 q))
;; (1)
#+END_SRC

- The macro =fresh= has a syntax similar to =run*=,
  but returns a goal with freshly created variables.
  It can be read as the existential quantifier.

#+BEGIN_SRC scheme
(define (appendo l s out)
  (conde
   [(== '() l) (== s out)]
   [(fresh [a d res]
      (== (cons a d) l)
      (== (cons a res) out)
      (appendo d s res))]))

(run* [r] (appendo '(1 2) '(3 4) r))
;; ((1 2 3 4))

(run* [p q] (appendo p q '(1 2 3 4)))
;; ((() (1 2 3 4))
;;  ((1) (2 3 4))
;;  ((1 2) (3 4))
;;  ((1 2 3) (4))
;;  ((1 2 3 4) ()))
#+END_SRC

- Goal constructors such as =appendo= are functions which return goals.
  They are conventionally named with an =o= at the end.

A literal and clumsy translation of the =appendo= example in /Prolog/ is as follows.

#+BEGIN_SRC prolog
appendo(L,S,Out) :- L = [], S = Out.
appendo(L,S,Out) :- L = [A|D], Out = [A|Res], appendo(D,S,Res).

?- appendo([1,2],[3,4],R).
%@ R = [1, 2, 3, 4].

?- appendo(P,Q,[1,2,3,4]).
%@ P = [],
%@ Q = [1, 2, 3, 4] ;
%@ P = [1],
%@ Q = [2, 3, 4] ;
%@ P = [1, 2],
%@ Q = [3, 4] ;
%@ P = [1, 2, 3],
%@ Q = [4] ;
%@ P = [1, 2, 3, 4],
%@ Q = [] ;
%@ false.
#+END_SRC

* <<sec-proposal>>Project proposal

** Where choice happens

We start with a [[https://github.com/ysmiraak/phynaster/blob/9c0e813833ed6bb1c78f89e46e249a4d6ccc9017/src/phynaster/logic2.clj#L85-L92][modified implementation]] of /miniKanren/.
Here we first discuss the important aspects of modifications,
and then give an example for where the neural network comes into play.

*** State board

In /miniKanren/, a goal is a function which takes an inference state as input and returns a transformed state.
The transformed state could be one of the following.

- A state, where the goal is satisfied.
- A thunk, which is a function of no argument, and when called returns a transformed state.
- A pair containing a state and a thunk.
- A failure.

The two user-interface macros =fresh= and =conde= automatically wraps inner goals in thunks to delay their effects.
When the results are being pulled (for instance by =run*=),
first the global goal is called on the initial state,
and then depending on the output,
further actions are taken.
If the output is a thunk, it means that no result has been reached yet.
The thunk is executed to expose the first subgoal to the current state,
which may spawn another thunk if more subgoals are to be called.
If the output is a pair, it means that the global goal contains some disjunction of subgoals,
and that one of the branches has reached a result.

This design, while simple, only allows for binary branching in general, and therefore, binary choices.
In our implementation, we used a record type =Board= to aggregate transformed states.
A board consists of three queues.

- A queue of states, which are the fruitful leaf nodes of the search tree.
- A queue of thunks, which are the unfinished branches, waiting to be explored.
- A queue of failed states, which exist only for the purpose of analysis.

If the thunk queue is empty, the search tree is fully explored.
Otherwise, to advance the search process,
we pop the first thunk and execute it.
If the first subgoal in the thunk (namely the current goal) is a disjunctive,
then more thunks may be spawned and appended to the thunk queue.
This way, we rotate and explore unfinished branches,
performing a breadth-first search.
Had we used a stack for storing thunks,
then a depth-first search would be performed.

As an example, the following global goal with five subgoals in a nested disjunctive form
will expose and explore those subgoals in their indexed order.

#+BEGIN_SRC clojure
(| g1
   (| (| g4
         g5)
      g3)
   g2)
#+END_SRC

To prioritize the exploration of difference branches,
we simply use a priority queue for storing thunks,
with the priorities assigned by the neural network model.

*** Descriptive goals

We added descriptions to the goals in the form of symbolic expressions,
which are used by the neural network model for predicting the probability of success.

The symbolic descriptions are constructed inductively as follows.

<<def-description>>
#+BEGIN_EXAMPLE
Description :=
| (ConstraintName Data ...)
| (& Description ...)
| (| Description ...)

ConstraintName :=
| ==
| !=
| domain
| ...

Data :=
| Constant
| Variable
| List

List := (list* Data ... Tail)
Tail := nil | Variable
#+END_EXAMPLE

The symbols in the language are from either
a finite pool of constants,
a finite pool of variables,
or a fixed pool of special symbols including constraint names, =&=, =|=, =list*=, and =nil=.

The constants and variables are atomic data.
Compound data are represented as lists.
Here we use a special list format =list*=,
which is a hybrid between =cons= and =list=.
On one hand,
the =cons= cell representation (e.g. =(cons a (cons b (cons c nil)))=) results in deeply nested expressions,
making it slower for the neural network to process,
and the gradient flow during backpropagation difficult.
On the other hand,
the flatten =list= representation (e.g. =(list a b c)=) lacks the ability to have a logical variable sitting as the tail,
which removes the usefulness of lists for representing partially generated expressions during logical inference.
Therefore, we adopt a hybrid representation =list*=,
translating =(cons a (cons b (cons c tail)))= as =(list* a b c tail)=
where =tail= is either =nil= or a logical variable.
This method gives us an efficient list representation retaining its inductive structure
which makes it expressive enough for representing arbitrary algebraic expressions.

A goal description is a symbolic expression whose head is a special symbol representing the goal constructor.
Goal constructors include constraints (equality ~==~, disequality ~!=~, finite domain =domain=, etc.)
and connectives (=&= and =|=).

*** <<sec-example>>Example

Here is a sample goal description.
For readability, we use keywords (=:p=, =:q=, and =:r=) to represent logical variables.

#+BEGIN_SRC clojure
(& (== (list* 1 :p :r) :q)
   (| (& (== :r nil)
         (== :p 2))
      (== :p 4))
   (!= (list* 1 2 nil) :q))
#+END_SRC

This global goal contains one disjunctive with two branches.
After initialization, the inference board contains two thunks,
each containing its current local goal with the respective description as follows.

#+BEGIN_SRC clojure
;; thunk 1
(& (== :r nil)
   (== :p 2))
;; thunk 2
(== :p 4)
#+END_SRC

We will run a neural network on the global goal description,
as well as the local goal descriptions,
to produce embedded representations in a shared vector space.
For each thunk,
the global embedding and the local embedding are then combined in the final output layer to predict its success rate.

A useful neural network should rank thunk 2 higher than thunk 1,
since thunk 1 will fails due to the disequality constraint,
but thunk 2 will produce one successful result.

#+BEGIN_SRC clojure
{:p 4, :q (1 4 . ?0), :r ?0}
#+END_SRC

Namely,
=:p= unifies with the constant =4=,
=:q= unifies with the list =(list* 1 4 :r)=,
and =:r= remains unbound.

*** Problem: recursive goals

Consider =nato=, a recursively defined goal.

#+BEGIN_SRC scheme
(define (nato n)
  (| (== 'z n)
     (fresh [m]
       (== (list* 's m) n)
       (nato m))))
#+END_SRC

While this goal can be constructed and executed without termination problems,
due to its recursive part being contained in a thunk,
the description of this goal, however, is an infinite expression.

#+BEGIN_SRC clojure
(| (== 'z :n)
   (& (== (list* 's :n1) :n)
      (| (== 'z :n1)
         (& (== (list* 's :n2) :n1)
            (| (== 'z :n2)
               (& (== (list* 's :n3) :n2)
                  ...))))))
#+END_SRC

We propose an ad hoc treatment to this problem:
We replace the recursive part of the description with a special expression =(rec Variable ...)=
where =rec= is a new special symbol.

This way, the goal =(nato :n)= has the following global description.

#+BEGIN_SRC clojure
(| (== 'z :n)
   (& (== (list* 's :n1) :n)
      (rec :n1)))
#+END_SRC

And the subgoal =(rec :n')= when dethunked will have the following description.

#+BEGIN_SRC clojure
(| (== 'z :n1)
   (& (== (list* 's :n2) :n1)
      (rec :n2)))
#+END_SRC

** Neural network

As mentioned in the [[sec-example][example]] above,
we will use a neural network to predict the success rate of a thunk based on its global and local goal descriptions.
Here we first describe a recursive attention network for embedding descriptions,
and then discuss the training process.

*** Description embedding

Following the [[def-description][definition of descriptions]] above,
we propose to embed each sub-expression recursively using one attention network.

We start with a learned embedding matrix for special symbols and atomic data types.
The embedding matrix consists of three regions,
respectively for special symbols, constants, and variables.
The number of special symbols are predetermined by the logic programming language,
crucially the available types of constraints.
Although an indefinite number of constants and variables are needed for solving problems of arbitrary complexities,
we have to sacrifice that flexibility in order to interface with the neural model.
Luckily, in practice, most problems require only a manageable amount of constants and variables.
The success rate of a goal should stay invariant under consistent permutations of variables.
This invariance must be learned to the model (see [[sec-training][training]]).
Likewise, with the limited types of constraints under consideration (equality, disequality, and finite domain),
the mapping of constants is permutable as well.

After the embedding lookup for the aforementioned atomic symbols,
we use the attention network to embed each compound expression =(expr-0 expr-1 ... expr-N)= recursively.
We follow the attention mechanism of [[https://arxiv.org/abs/1706.03762][Vaswani et al. (2017)]]
and apply it in two stages.
Firstly, self-attention allows each sub-expression to query all other sub-expressions,
producing \(N+1\) vectors, which are sometimes known as annotations in the neural machine translation literature.
Secondly, we take the annotation for the head expression =expr-0= to query the annotations for the other sub-expressions,
producing the final embedding vector for the whole expression.

One major characteristic of attention networks in comparison to the alternatives such as recurrent or convolutional networks
is that they are naturally positional invariant.
For instance, there is no difference in processing ~(== p q)~ versus ~(== q p)~,
or =(& f g)= versus =(& g f)=.
This property is a big advantage since all goal constructors under our consideration are commutative.
However, for lists, such as =(list* a b c ... tail)=,
the sequential ordering of sub-expressions are important.
Here we apply the commonly used sinusoidal position encoding.

*** <<sec-training>>Training

The neural network is trained to predict the success rate of a thunk in a supervised manner.

In the simple case, a thunk when executed either fails or produces a successful state.
The target for prediction is binary.
However, when the thunk contains disjunctive goals,
then more thunks will be produced,
some of which may fail, and others succeed.
As long as all the spawned thunks terminate,
we can still take the success ratio,
and train the model to minimize binary cross entropy.

In the worst case, the thunk may recursively spawn infinitely many,
or a finite but intractable number of thunks.
Then the success rate has to be estimated.
Techniques from reinforcement learning can be useful here,
such as Monte Carlo tree search.
However, we do not expect this to happen for problems under our consideration.
Note that not all recursive goals produce infinitely spawning thunks,
since some constraints may fail the recursive branch.
Problems in practice can usually be described with constraints
which make the solution space finite,
and improve the efficiency of search.

One major obstacle in training a recursive model is that the training examples are difficult to batch.
We propose an experimental method for mini-batching.
Each batch is produced from just one example,
but with randomized mappings for constants and variables.
The model should learn the invariance of mapping permutation,
and rely on no additional information in the embedding apart from the different identities of constants and variables.
The effectiveness of this method is up to investigation.
We will examine the embedding matrix and the attention alignment to analyse what the model learns,
and adjust our method accordingly.

** Evaluation

The purpose of the neural network model is to assign weights to choices,
so that the logical inference engine could prioritize branches which are more likely to succeed.

We propose two metrics for evaluation.

1. The number of dead ends (failure states) met before finding the first successful result.
2. The number of choices made in producing the first successful result.

These two metrics reflect the cost of inference regarding respectively the breadth of search and the depth of search.

We will reserve examples from each problem category,
and compare the metrics when running inference on them
with and without the neural network model.

* Timeline planning

** June

- find adviser and refine project idea
- determine problem datasets
- complete minikanren implementation
- produce training data
- register thesis

** July & August

- train neural network
- evaluation and introspection
- adjustments and further experiments

** September

- finish thesis paper
