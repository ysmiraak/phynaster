#+TITLE: The road first taken: Optimizing choices in logical inference with neural network
#+DATE: May 30, 2020
#+AUTHOR: Kuan Yu

This is a project proposal for my master's thesis in the [[http://www.ling.uni-potsdam.de/cogsys/][Cognitive Systems]] program at the University of Potsdam.
First draft on May 30, 2020.

* Introduction

Logical inference commonly involves a backward search process,
where we check each rule from which the goal can be derived.
The plurality of rules creates branches in the search tree,
and the complexity of leveled branches multiplies.
When we come to a branching node, we face choices.
Some choices eventually lead us to successful results,
and others to dead ends,
or worse, to infinite loops.

If we wish to retrieve all possible results,
an exhaustive search is unavoidable,
in which case the choices are irrelevant,
since all branches must be traversed, preferably in parallel.
However, in practice, we often need just one successful result which satisfies our goal.
Moreover, in reality, making a wrong choice has far more severe consequences than the computation cost.
For the time being, we guide machines with well-defined goals whose results can be simulated,
but eventually they will have to learn by interacting with the real world.

We propose to augment a general logical inference engine with a neural network responsible for making choices:
a symbolic intelligence with a connectionist brain.
This project will be a rudimentary exploration into this idea.
We will train a neural network in a supervised manner to predict the success rate of choices for prioritization
in order to reduce the breadth and the depth of the subtree traversed before the first successful result is found.

This project aims at laying a solid and extensible groundwork for the following future research directions.
- Replacing supervised training with reinforcement learning, so that the program learns to make better choices through interactions.
- Replacing success rate with more complicated objectives for which the choices are to be optimized.

* Background

** Neural guided logical inference

[[https://arxiv.org/abs/1701.06972][Loos et al. (2017)]] demonstrated the usefulness of deep neural networks for guiding first-order logic proof search,
where the networks were used for ranking clause selections in the [[https://wwwlehre.dhbw-stuttgart.de/~sschulz/E/E.html][E theorem prover]],
which significantly reduced the number of proof search steps.
They experimented with convolutional networks and tree LSTM networks.
[[https://arxiv.org/abs/1805.07563][Kaliszyk et al. (2018)]] successfully applied reinforcement learning for first-order theorem proving,
but with tableau calculus for finding refutational proofs.
[[https://arxiv.org/abs/1811.00796][Kusumoto et al. (2018)]] trained graph networks with reinforcement learning
for guiding the construction of proof trees in intuitionistic propositional logic using sequent calculus,
which outperformed Coq's /tauto/ tactic.
These works have shown the power of neural guidance for logical inference in limited contexts.

[[https://arxiv.org/abs/1805.10872][Manhaeve et al. (2018)]] introduced /DeepProbLog/
which integrated neural predicates, e.g. images, into probabilistic logic programming.
[[https://arxiv.org/abs/1901.04195][Marra et al. (2019)]] used graph networks in probabilistic graphical models
with MAP inference for modeling logical reasoning.
These works explored the integration of probabilistic modeling and logical reasoning.
The general direction of their approach, however, diverges from ours.
Our goal is to harness the power of statistical methods
while retaining the definitive nature of symbolic reasoning.

Our approach is heavily inspired by [[https://arxiv.org/abs/1809.02840][Zhang et al. (2018)]]
who showed the benefits of neural guidance for program synthesis.
They experimented with recurrent LSTM networks and gated graph networks
for synthesizing /Scheme/ programs using the /miniKanren/ logic programming system.
The neural networks were trained with supervised curriculum and scheduled sampling
to predict the ground-truth optimal candidates at each step for expanding the partially synthesized program.
We discuss the following details in their work from which [[sec-proposal][our proposal]] differs.

- They used neural networks for embedding constraints which depended on the target synthesis language.
  The synthesis engine consisted of a few relational programs (e.g. =evalo=, =lookupo=)
  each containing several candidate branches of constraints.
  The weights of the networks were specialized for each relational program,
  and the model predicts a distribution over the candidates in that program.
- Their recurrent model was more efficient and performant, but it discarded variable identities.
- During test time, they used a greedy policy.  Only the best predicted candidate was chosen.

** The miniKanren language

TODO

*** Basic examples

#+BEGIN_SRC scheme
(run* [q] (== 1 q))
;; (1)
#+END_SRC

#+BEGIN_SRC scheme
(run* [q]
  (conde
   [(== 0 q)]
   [(== 1 q)]))
;; (0 1)
#+END_SRC

#+BEGIN_SRC scheme
(define (appendo l s out)
  (conde
   [(== '() l) (== s out)]
   [(fresh [a d res]
      (== (cons a d) l)
      (== (cons a res) out)
      (appendo d s res))]))

(run* [p q] (appendo p q '(1 2 3 4)))
;; ((() (1 2 3 4))
;;  ((1) (2 3 4))
;;  ((1 2) (3 4))
;;  ((1 2 3) (4))
;;  ((1 2 3 4) ()))
#+END_SRC

Here is a faithful albeit non-idiomatic and clumsy translation in /Prolog/.

#+BEGIN_SRC prolog
append(L,S,Out) :- L = [], S = Out.
append(L,S,Out) :- L = [A|D], Out = [A|Res], append(D,S,Res).

?- append(P,Q,[1,2,3,4]).
%@ P = [],
%@ Q = [1, 2, 3, 4] ;
%@ P = [1],
%@ Q = [2, 3, 4] ;
%@ P = [1, 2],
%@ Q = [3, 4] ;
%@ P = [1, 2, 3],
%@ Q = [4] ;
%@ P = [1, 2, 3, 4],
%@ Q = [] ;
%@ false.
#+END_SRC

#+BEGIN_SRC scheme
(run* [r] (appendo '(1 2) '(3 4) r))
;; ((1 2 3 4))
#+END_SRC

the importance of recursion

#+BEGIN_SRC scheme
(define (nato n)
  (conde
   [(== 'z n)]
   [(fresh [m]
      (== (cons 's m) n)
      (nato m))]))

(run 4 [q] (nato q))
;; (z (s . z) (s s . z) (s s s . z))
#+END_SRC

#+BEGIN_SRC scheme
(define (addo m n o)
  (conde
   [(== 'z m) (== n o)]
   [(fresh [l p]
      (== (cons 's l) m)
      (== (cons 's p) o)
      (addo l n p))]))

(run* [q] (addo '(s . z) '(s . z) q))
;; ((s s . z))
#+END_SRC

*** Why miniKanren?

- general recursion Ã  la /Prolog/
- it's an extensible embedded language, with numerous extensions
- minimal core (microKanren), easy to implement and extend
- breath first search instead of depth first search, with one line of change

#+BEGIN_SRC scheme
(define (bind $ g)
  (cond
    ((null? $) mzero)
    ((procedure? $) (lambda () (bind ($) g)))
    (else (mplus (g (car $)) (bind (cdr $) g)))))
#+END_SRC

* <<sec-proposal>>Project proposal

** Where magic happens

We start with an [[https://github.com/ysmiraak/phynaster/blob/9c0e813833ed6bb1c78f89e46e249a4d6ccc9017/src/phynaster/logic2.clj#L85-L92][modified implementation]] of miniKanren.
Of crucial interests are the following two aspects of modifications.

*** Flattened disjunctives

TODO

#+BEGIN_SRC clojure
(| g1
   (| (| g4
         g5)
      g3)
   g2)
#+END_SRC

*** Descriptive goals

We added descriptions to the goals in the form of symbolic expressions,
which are used by the neural network model for predicting the probability of success.

The symbolic descriptions are constructed inductively as follows.

<<def-description>>
#+BEGIN_EXAMPLE
Description :=
| (ConstraintName Data ...)
| (& Description ...)
| (| Description ...)

ConstraintName :=
| ==
| !=
| domain
| ...

Data :=
| Constant
| Variable
| List

List := (list* Data ... Tail)
Tail := nil | Variable
#+END_EXAMPLE

The symbols in the language are from either
a finite pool of constants,
a finite pool of variables,
or a fixed pool of special symbols including constraint names, =&=, =|=, =list*=, and =nil=.

The constants and variables are atomic data.
Compound data are represented as lists.
Here we use a special list format =list*=,
which is a hybrid between =cons= and =list=.
On one hand,
the =cons= cell representation (e.g. =(cons a (cons b (cons c nil)))=) results in deeply nested expressions,
making it slower for the neural network to process,
and the gradient flow during backpropagation difficult.
On the other hand,
the flatten =list= representation (e.g. =(list a b c)=) lacks the ability to have a logical variable sitting as the tail,
which removes the usefulness of lists for representing partially generated expressions during logical inference.
Therefore, we adopt a hybrid representation =list*=,
translating =(cons a (cons b (cons c tail)))= as =(list* a b c tail)=
where =tail= is either =nil= or a logical variable.
This method gives us an efficient list representation retaining its inductive structure
which makes it expressive enough for representing arbitrary algebraic expressions.

A goal description is a symbolic expression whose head is a special symbol representing the goal constructor.
Goal constructors include constraints (equality ~==~, disequality ~!=~, finite domain =domain=, etc.)
and connectives (=&= and =|=).

*** <<sec-example>>Example

Here is a sample goal description.
For readability, we use keywords (=:p=, =:q=, and =:r=) to represent logical variables.

#+BEGIN_SRC clojure
(& (== (list* 1 :p :r) :q)
   (| (& (== :r nil)
         (== :p 2))
      (== :p 4))
   (!= (list* 1 2 nil) :q))
#+END_SRC

This global goal contains one disjunctive with two branches.
After initialization, the inference board contains two thunks,
each containing its current local goal with the respective description as follows.

#+BEGIN_SRC clojure
;; thunk 1
(& (== :r nil)
   (== :p 2))
;; thunk 2
(== :p 4)
#+END_SRC

We will run a neural network on the global goal description,
as well as the local goal descriptions,
to produce embedded representations in a shared vector space.
For each thunk,
the global embedding and the local embedding are then combined in the final output layer to predict its success rate.

A useful neural network should rank thunk 2 higher than thunk 1,
since thunk 1 will fails due to the disequality constraint,
but thunk 2 will produce one successful result.

#+BEGIN_SRC clojure
{:p 4, :q (1 4 . ?0), :r ?0}
#+END_SRC

Namely,
=:p= unifies with the constant =4=,
=:q= unifies with the list =(list* 1 4 :r)=,
and =:r= remains unbound.

*** Problem: recursive goals

Consider =nato=, a recursively defined goal.

#+BEGIN_SRC scheme
(define (nato n)
  (| (== 'z n)
     (fresh [m]
       (== (list* 's m) n)
       (nato m))))
#+END_SRC

While this goal can be constructed and executed without termination problems,
due to its recursive part being contained in a thunk,
the description of this goal, however, is an infinite expression.

#+BEGIN_SRC clojure
(| (== 'z :n)
   (& (== (list* 's :n') :n)
      (| (== 'z :n')
         (& (== (list* 's :n'') :n')
            (| (== 'z :n'')
               (& (== (list* 's :n''') :n'')
                  ...))))))
#+END_SRC

We propose an ad hoc treatment to this problem:
We replace the recursive part of the description with a special expression =(rec Variable ...)=
where =rec= is a new special symbol.

This way, the goal =(nato :n)= has the following global description.

#+BEGIN_SRC clojure
(| (== 'z :n)
   (& (== (list* 's :n') :n)
      (rec :n')))
#+END_SRC

And the subgoal =(rec :n')= when dethunked will have the following description.

#+BEGIN_SRC clojure
(| (== 'z :n')
   (& (== (list* 's :n'') :n')
      (rec :n'')))
#+END_SRC

** Neural network

As mentioned in the [[sec-example][example]] above,
we will use a neural network to predict the success rate of a thunk based on its global and local goal descriptions.
Here we first describe a recursive attention network for embedding descriptions,
and then discuss the training process.

*** Description embedding

Following the [[def-description][definition of descriptions]] above,
we propose to embed each sub-expression recursively using one attention network.

We start with a learned embedding matrix for special symbols and atomic data types.
The embedding matrix consists of three regions,
respectively for special symbols, constants, and variables.
The number of special symbols are predetermined by the logic programming language,
crucially the available types of constraints.
Although an indefinite number of constants and variables are needed for solving problems of arbitrary complexities,
we have to sacrifice that flexibility in order to interface with the neural model.
Luckily, in practice, most problems require only a manageable amount of constants and variables.
The success rate of a goal should stay invariant under consistent permutations of variables.
This invariance must be learned to the model (see [[sec-training][training]]).
Likewise, with the limited types of constraints under consideration (equality, disequality, and finite domain),
the mapping of constants is permutable as well.

After the embedding lookup for the aforementioned atomic symbols,
we use the attention network to embed each compound expression =(expr-0 expr-1 ... expr-N)= recursively.
We follow the attention mechanism of [[https://arxiv.org/abs/1706.03762][Vaswani et al. (2017)]]
and apply it in two stages.
Firstly, self-attention allows each sub-expression to query all other sub-expressions,
producing \(N+1\) vectors, which are sometimes known as annotations in the literature.
Secondly, we take the annotation for the head expression =expr-0= to query the annotations for the other sub-expressions,
producing the final embedding vector for the whole expression.

One major characteristic of attention networks in comparison to the alternatives such as recurrent or convolutional networks
is that they are naturally positional invariant.
For instance, there is no difference in processing ~(== p q)~ versus ~(== q p)~,
or =(& f g)= versus =(& g f)=.
This property is a big advantage since all goal constructors under our consideration are commutative.
However, for lists, such as =(list* a b c ... tail)=,
the sequential ordering of sub-expressions are important.
Here we apply the commonly used sinusoidal position encoding.

*** <<sec-training>>Training

The neural network is trained to predict the success rate of a thunk in a supervised manner.

In the simple case, a thunk when executed either fails or produces a successful state.
The target for prediction is binary.
However, when the thunk contains disjunctive goals,
then more thunks will be produced,
some of which may fail, and others succeed.
As long as all the spawned thunks terminate,
we can still take the success ratio,
and train the model to minimize binary cross entropy.

In the worst case, the thunk may recursively spawn infinitely many,
or a finite but intractable number of thunks.
Then the success rate has to be estimated.
Techniques from reinforcement learning may be useful here,
such as Monte Carlo tree search.
However, we do not expect this to happen for problems under our consideration.
Note that not all recursive goals produce infinitely spawning thunks,
since some constraints may fail the recursive branch.
Problems in practice can usually be described with constraints
which make the solution space finite,
and improve the efficiency of search.

One major obstacle in training a recursive model is that the training examples are difficult to batch.
We propose an experimental method for mini-batching.
Each batch is produced from just one example,
but with randomized mappings for constants and variables.
The model should learn the invariance of mapping permutation,
and rely on no additional information in the embedding apart from the different identities of constants and variables.
The effectiveness of this method is up to investigation.
We will examine the embedding matrix and the attention alignment to analyse what the model learns,
and adjust our method accordingly.

** Evaluation

The purpose of the neural network model is to assign weights to choices,
so that the logical inference engine could prioritize branches which are more likely to succeed.

We propose two metrics for evaluation.

1. The number of dead ends (failure states) met before finding the first successful result.
2. The number of choices made in producing the first successful result.

These two metrics reflect the cost of inference regarding respectively the breadth of search and the depth of search.

We will reserve examples from each problem category,
and compare the metrics when running inference on them
with and without the neural network model.

* Timeline planning

** June

- Find adviser and refine project idea.
- Determine problem datasets.
- Complete miniKanren implementation.
- Produce training data.
- Register thesis.

** July & August

- Train neural network.
- Evaluation and introspection.
- Adjustments and further experiments.

** September

- Finish thesis paper.
