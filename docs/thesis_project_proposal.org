#+TITLE: The road first taken: Optimizing choices in logical inference with neural network
#+DATE: May 30, 2020
#+AUTHOR: Kuan Yu

This is a project proposal for my master's thesis in the [[http://www.ling.uni-potsdam.de/cogsys/][Cognitive Systems]] program at the University of Potsdam.
First draft on May 30, 2020.

* Introduction

TODO

symbolic intelligence with connectionist brain

* Background

** Neural guided logical inference

TODO

** The miniKanren language

TODO

*** Basic examples

#+BEGIN_SRC scheme
(run* [q] (== 1 q))
;; (1)
#+END_SRC

#+BEGIN_SRC scheme
(run* [q]
  (conde
   [(== 0 q)]
   [(== 1 q)]))
;; (0 1)
#+END_SRC

#+BEGIN_SRC scheme
(define (appendo l s out)
  (conde
   [(== '() l) (== s out)]
   [(fresh [a d res]
      (== (cons a d) l)
      (== (cons a res) out)
      (appendo d s res))]))

(run* [p q] (appendo p q '(1 2 3 4)))
;; ((() (1 2 3 4))
;;  ((1) (2 3 4))
;;  ((1 2) (3 4))
;;  ((1 2 3) (4))
;;  ((1 2 3 4) ()))
#+END_SRC

Here is a faithful albeit non-idiomatic and clumsy translation in Prolog.

#+BEGIN_SRC prolog
append(L,S,Out) :- L = [], S = Out.
append(L,S,Out) :- L = [A|D], Out = [A|Res], append(D,S,Res).

?- append(P,Q,[1,2,3,4]).
%@ P = [],
%@ Q = [1, 2, 3, 4] ;
%@ P = [1],
%@ Q = [2, 3, 4] ;
%@ P = [1, 2],
%@ Q = [3, 4] ;
%@ P = [1, 2, 3],
%@ Q = [4] ;
%@ P = [1, 2, 3, 4],
%@ Q = [] ;
%@ false.
#+END_SRC

#+BEGIN_SRC scheme
(run* [r] (appendo '(1 2) '(3 4) r))
;; ((1 2 3 4))
#+END_SRC

the importance of recursion

#+BEGIN_SRC scheme
(define (nato n)
  (conde
   [(== 'z n)]
   [(fresh [m]
      (== (cons 's m) n)
      (nato m))]))

(run 4 [q] (nato q))
;; (z (s . z) (s s . z) (s s s . z))
#+END_SRC

#+BEGIN_SRC scheme
(define (addo m n o)
  (conde
   [(== 'z m) (== n o)]
   [(fresh [l p]
      (== (cons 's l) m)
      (== (cons 's p) o)
      (addo l n p))]))

(run* [q] (addo '(s . z) '(s . z) q))
;; ((s s . z))
#+END_SRC

*** Why miniKanren?

- general recursion Ã  la Prolog
- it's an extensible embedded language, with numerous extensions
- minimal core (microKanren), easy to implement and extend
- breath first search instead of depth first search, with one line of change

#+BEGIN_SRC scheme
(define (bind $ g)
  (cond
    ((null? $) mzero)
    ((procedure? $) (lambda () (bind ($) g)))
    (else (mplus (g (car $)) (bind (cdr $) g)))))
#+END_SRC

** Attention network

TODO

* Project proposal

** Where magic happens

We start with an [[https://github.com/ysmiraak/phynaster/blob/9c0e813833ed6bb1c78f89e46e249a4d6ccc9017/src/phynaster/logic2.clj#L85-L92][modified implementation]] of miniKanren.
Of crucial interests are the following two aspects of modifications.

*** Flattened disjunctives

TODO

#+BEGIN_SRC clojure
(| g1
   (| (| g4
         g5)
      g3)
   g2)
#+END_SRC

*** Descriptive goals

We added descriptions to the goals in the form of symbolic expressions,
which are used by the neural network model for predicting the probability of success.

The symbolic descriptions are constructed inductively as follows.

<<def-description>>
#+BEGIN_EXAMPLE
Description =
| (ConstraintName Data ...)
| (& Description ...)
| (| Description ...)

ConstraintName =
| ==
| !=
| domain
| ...

Data =
| Constant
| Variable
| List

List = (list* Data ... Tail)
Tail = nil | Variable
#+END_EXAMPLE

The symbols in the language are from either
a finite pool of constants,
a finite pool of variables,
or a fixed pool of special symbols including constraint names, =&=, =|=, =list*=, and =nil=.

The constants and variables are atomic data.
Compound data are represented as lists.
Here we use a special list format =list*=,
which is a hybrid between =cons= and =list=.
On one hand,
the =cons= cell representation (e.g. =(cons a (cons b (cons c nil)))=) results in deeply nested expressions,
making it slower for the neural network to process,
and the gradient flow during backpropogation difficult.
On the other hand,
the flatten =list= representation (e.g. =(list a b c)=) lacks the ability to have a logical variable sitting as the tail,
which removes the usefulness of lists for representing partially generated expressions during logical inference.
Therefore, we adopt a hybrid representation =list*=,
translating =(cons a (cons b (cons c tail)))= as =(list* a b c tail)=
where =tail= is either =nil= or a logical variable.
This method gives us an efficient list representation retaining its inductive structure
which makes it expressive enough for representating arbitrary algebraic expressions.

A goal description is a symbolic expression whose head is a special symbol representing the goal constructor.
Goal constructors include constraints (equality ~==~, disequality ~!=~, finite domain =domain=, etc.)
and connectives (=&= and =|=).

*** <<sec-example>>Example

Here is a sample goal description.
For readability, we use keywords (=:p=, =:q=, and =:r=) to represent logical variables.

#+BEGIN_SRC clojure
(& (== (list* 1 :p :r) :q)
   (| (& (== :r nil)
         (== :p 2))
      (== :p 4))
   (!= (list* 1 2 nil) :q))
#+END_SRC

This global goal contains one disjunctive with two branches.
After initialization, the inference board contains two thunks,
each containing its current local goal with the respective description as follows.

#+BEGIN_SRC clojure
;; thunk 1
(& (== :r nil)
   (== :p 2))
;; thunk 2
(== :p 4)
#+END_SRC

We will run a neural network on the global goal description,
as well as the local goal descriptions,
to produce embedded representations in a shared vector space.
For each thunk,
the global embedding and the local embedding are then combined in the final output layer to predict its success rate.

A useful neural network should rank thunk 2 higher than thunk 1,
since thunk 1 will fails due to the disequality constraint,
but thunk 2 will produce one successful result.

#+BEGIN_SRC clojure
{:p 4, :q (1 4 . ?0), :r ?0}
#+END_SRC

Namely,
=:p= unifies with the constant =4=,
=:q= unifies with the list =(list* 1 4 :r)=,
and =:r= remains unbound.

*** Problem: recursive goals

Consider =nato=, a recursively defined goal.

#+BEGIN_SRC scheme
(define (nato n)
  (| (== 'z n)
     (fresh [m]
       (== (list* 's m) n)
       (nato m))))
#+END_SRC

While this goal can be constructed and executed without termination problems,
due to its recursive part being contained in a thunk,
the description of this goal, however, is an infinite expression.

#+BEGIN_SRC clojure
(| (== 'z :n)
   (& (== (list* 's :n') :n)
      (| (== 'z :n')
         (& (== (list* 's :n'') :n')
            (| (== 'z :n'')
               (& (== (list* 's :n''') :n'')
                  ...))))))
#+END_SRC

We propose an ad hoc treatment to this problem:
We replace the recusive part of the description with a special expression =(rec Variable ...)=
where =rec= is a new special symbol.

This way, the goal =(nato :n)= has the following global description.

#+BEGIN_SRC clojure
(| (== 'z :n)
   (& (== (list* 's :n') :n)
      (rec :n')))
#+END_SRC

And the subgoal =(rec :n')= when dethunked will have the following description.

#+BEGIN_SRC clojure
(| (== 'z :n')
   (& (== (list* 's :n'') :n')
      (rec :n'')))
#+END_SRC

** Neural network

As mentioned in the [[sec-example][example]] above,
we will use a neural network to predict the success rate of a thunk based on its global and local goal descriptions.
Here we first describe a recursive attention network for embedding descriptions,
and then discuss the training process.

*** Description embedding

Following the [[def-description][definition of descriptions]] above,
we propose to embed each sub-expression recursively using one attention network.

We start with a learned embedding matrix for special symbols and atomic data types.
The embedding matrix consists of three regions,
respectively for special symbols, constants, and variables.
The number of special symbols are predetermined by the logical programming language,
crucially the available types of constraints.
Although an indefinite number of constants and variables are needed for solving problems of arbitrary complexities,
we have to sacrifice that flexibility in order to interface with the neural model.
Luckily, in practice, most problems require only a manageable amount of constants and variables.
The success rate of a goal should stay invariant under consistent permutations of variables.
This invariance must be learned to the model (see [[sec-training][training]]).
Likewise, with the limited types of constraints under consideration (equality, disequality, and finite domain),
the mapping of constants is permutable as well.

After the embedding lookup for the aforementioned atomic symbols,
we use the attention network to embed each compound expression =(expr-0 expr-1 ... expr-N)= recursively.
The attention network consists of two stages.
Firstly, self-attention allows each sub-expression to query all other sub-expressions,
producing \(N+1\) vectors, which are sometimes known as annotations in the literature.
Secondly, we take the annotation for the head expression =expr-0= to query the annotations for the other sub-expressions,
producing the final embedding vector for the whole expression.

One major characteristic of attention networks in comparison to the alternatives such as recurrent or convolutional networks
is that they are naturally positional invariant.
For instance, there is no difference in processing ~(== p q)~ versus ~(== q p)~,
or =(& f g)= versus =(& g f)=.
This property is a big advantage since all goal constructors under our consideration are commutative.
However, for lists, such as =(list* a b c ... tail)=,
the sequential ordering of sub-expressions are important.
Here we apply the commonly used sinusoidal position encoding.

*** <<sec-training>>Training

TODO

- exhaustive search vs monte carlo tree search
- binary prediction

** Evaluation

TODO

* Timeline planning

TODO
